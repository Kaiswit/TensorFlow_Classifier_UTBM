# -*- coding: utf-8 -*-
"""UE4 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zTVY7mNuDxfL-8JRDSt1Jy0XBYjr43w7

# Deep Learning project: Image Classifier

The aim of this project is, given a special image dataset, to build up an image classifier that is able to recongnize a man or a woman on an image. Eventually, we can push further to recognize features on the faces present on a specific image. To do so, we'll firstly prepare all the data and then build up a first network and see what it does. Then we'll progressively improve this first model to obtain a more performant one. An other thing, we mainly focused on the men/women classification because we didn't have too much time to implement the multi classes implementation but we have the theory (like changing sigmoid to softmax, increase the number of classes etc)
"""

# Importing all needed librairies
import os
import shutil
import io

import pandas as pd
import tensorflow as tf
import matplotlib as pyplt
import numpy as np
from skimage import io
from PIL import Image

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler

"""## Data treatment

As a data scientist, the first step before really building a model is analysing the data we'll use for our work. For this project the dataset is pretty huge and clean so we won't have too much to do to understand it. What we'll do in this part is open the csv, make the testing and training sets and eventually show some image:
"""

# Getting the dataset and creating folders
!rm -rf sample_data/
!rm -rf data/
!mkdir data
!wget http://info.iut-bm.univ-fcomte.fr/staff/couturie/img_align_celeba_small.zip
!unzip img_align_celeba_small.zip -d data/
!mv data/img_align_celeba_small/list_attr_celeba_small.csv data/list_attr.csv
!mv data/img_align_celeba_small/img_align_celeba_small/ data/img
!rm -rf data/img_align_celeba_small img_align_celeba_small.zip
!mkdir data/test/ data/train/

"""As their is a lot of images in a big format, we need to transform them to grayscale and resize them to reduce the memory space they use."""

# Transforming images to be able to open them without memory problems
images = os.listdir('data/img')
for i in images:
  img = Image.open(os.path.join('data/img/', i)).convert('L')
  img = img.resize((80, 80))
  img.save(os.path.join('data/img/', os.path.splitext(i)[0] + '.png'))

# Deleting every jpg images in the img folder
!rm -rf data/img/*.jpg

"""After that we can simply split the data in training and testing sets"""

# Reading dataset
data = pd.read_csv('data/list_attr.csv')

# Renaming 'Male' Column in 'Gender'
data.rename(columns={'Male': 'Gender'}, inplace=True)

# Replace all -1 by 0
data.replace(-1, 0, inplace=True)

# Change images refererences in the first column to match the transformer images
for i in range(len(data['image_id'])):
  data['image_id'] = data['image_id'].replace(data['image_id'][i], os.path.splitext(data['image_id'][i])[0] + '.png')

def copyfiles(file_list, src: str, dest: str):
  """ Method that copy every img of a list in path to a specified destination """
  for curr_file in file_list:
    shutil.copyfile(
        os.path.join(src, curr_file), 
        os.path.join(dest, curr_file)
    )
# Setting to display all columns
pd.set_option("display.max_columns", None)

# Sorting images 
path = ['data/train/', 'data/test/']
for i in range(2):
  images = data[(data['Eval'] == i)]
  print(images.head())
  copyfiles(images['image_id'], 'data/img', path[i])

def split_data(data):
  """ Method that split the given data in class and label """
  x = []
  y = []
  for d in data.to_numpy():
    # Opening images and storing them as numpy array
    x.append(np.array(pyplt.pyplot.imread(os.path.join('data/img', d[0]))).astype('float32'))
    # Converting results vectors as numpy array
    y.append(d[2])
  return x,y

# Creating train and test data set for men and women
train_x, train_y = split_data(data[(data['Eval'] == 0)])
test_x, test_y = split_data(data[(data['Eval'] == 1)])

def count_gender(data):
  """ Method that count number of male and female image in the given dataset """
  male: int = 0
  female: int = 0
  for d in data:
    if d == 1:
      male += 1
    else:
      female += 1
  return [male, female]

print(train_y)
# Repartition of men and woman images in train and test set
fig = pyplt.pyplot.figure()
ax = fig.add_axes([0, 0, 1, 1])
labels = ['Male (Test)', 'Female (Test)', 'Male (Train)', 'Female (Train)']
counts_test = count_gender(test_y)
counts_train = count_gender(train_y)
print("Test values: ", counts_test)
print("Train values: ", counts_train)
ax.bar(labels[0],counts_test[0], color ='orange')
ax.bar(labels[1],counts_test[1], color ='limegreen')
ax.bar(labels[2],counts_train[0], color ='r')
ax.bar(labels[3],counts_train[1], color ='b')
pyplt.pyplot.show()

"""We can see here that the train set is much bigger than the test set, and that in both sets there are more women images than men's"""

# Printing some images to see what they looks like
images = os.listdir('data/img')
for i in range(6):
  pyplt.pyplot.figure()
  pyplt.pyplot.imshow(pyplt.pyplot.imread(os.path.join('data/img', images[i])),  cmap=pyplt.cm.gray)

"""## Building networks

Now that we have imported and sorted our images, it's time to build our networks. For each test we have made their will be a section that will explain our choices. We applied a well know methodology: at each test we'll change only one parameter to study it's influence on the network performances and then we'll combine some parameters and make some conclusion about it.
"""

# Defining some usefull variables for every networks first
batch_size = 128 # Number of images in each batch
num_classes = 2 # Number of possible classes
epochs = 10 # Number of training steps
image_size = (80, 80, 1) # Images size
# Defining an early stopping callback that stop the trainin of the network if no 
# significative improvements have been made during 3 epochs
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)

"""### First network

For the first network we choose to implement a classic CNN architecture (a succession of Conv2D and MaxPooling layers) with not much layers to see if it's mandatory to have a deep network to have good results.
"""

model = Sequential()
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=image_size))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu')
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='sigmoid'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(learning_rate=1e-3),
              metrics=['accuracy'])

# Taking only a part of the images to reduce the training time
train_x_final = np.asarray(train_x)[:5000]
test_x_final = np.asarray(test_x)
train_y_vector = to_categorical(train_y, num_classes=2)[:5000]
test_y_vector = to_categorical(test_y, num_classes=2)

# Train the model using the parameters defined earlier
history = model.fit(train_x_final.reshape(train_x_final.shape[0], train_x_final.shape[1], train_x_final.shape[2], 1), train_y_vector,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector),
                    callbacks = [callback])

# Finally, evaluate the model using the test data           
score = model.evaluate(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""So we obtained pretty good results with this first network ! No, let's try to upgrade it by changing parameters and see what it does.

### Second network

In this network we choose to increase the number of fully connected Dense layer to see if they really improve the results !
"""

model = Sequential()
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=image_size))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu')
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(num_classes, activation='sigmoid'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(learning_rate=1e-3),
              metrics=['accuracy'])

# Taking only a part of the images to reduce the training time
train_x_final = np.asarray(train_x)[:5000]
test_x_final = np.asarray(test_x)
train_y_vector = to_categorical(train_y, num_classes=2)[:5000]
test_y_vector = to_categorical(test_y, num_classes=2)

# Train the model using the parameters defined earlier
history = model.fit(train_x_final.reshape(train_x_final.shape[0], train_x_final.shape[1], train_x_final.shape[2], 1), train_y_vector,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector),
                    callbacks = [callback])

# Finally, evaluate the model using the test data           
score = model.evaluate(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""If compare this architecture to the previous one, we see that the validation 
accuracy is smaller than the previous one. So it's not necessarily usefull
to add some fully connected Dense layers to improve the network like this.

### Third network

In this test we'll take the first network and add some dropout to see the influence of this layer one the network performance.
"""

model = Sequential()
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=image_size))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.15))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu')
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.15))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='sigmoid'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(learning_rate=1e-3),
              metrics=['accuracy'])

# Taking only a part of the images to reduce the training time
train_x_final = np.asarray(train_x)[:5000]
test_x_final = np.asarray(test_x)
train_y_vector = to_categorical(train_y, num_classes=2)[:5000]
test_y_vector = to_categorical(test_y, num_classes=2)

# Train the model using the parameters defined earlier
history = model.fit(train_x_final.reshape(train_x_final.shape[0], train_x_final.shape[1], train_x_final.shape[2], 1), train_y_vector,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector),
                    callbacks = [callback])

# Finally, evaluate the model using the test data           
score = model.evaluate(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""This network has nearly the same performances as the first one with a droupout level that is usually used.

### Fourth network

In this try we'll see if combine Dense layers and Dropout make significative changes on the network performances
"""

model = Sequential()
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=image_size))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.15))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.15))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(num_classes, activation='sigmoid'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(learning_rate=1e-3),
              metrics=['accuracy'])

# Taking only a part of the images to reduce the training time
train_x_final = np.asarray(train_x)[:5000]
test_x_final = np.asarray(test_x)
train_y_vector = to_categorical(train_y, num_classes=2)[:5000]
test_y_vector = to_categorical(test_y, num_classes=2)

# Train the model using the parameters defined earlier
history = model.fit(train_x_final.reshape(train_x_final.shape[0], train_x_final.shape[1], train_x_final.shape[2], 1), train_y_vector,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector),
                    callbacks = [callback])

# Finally, evaluate the model using the test data           
score = model.evaluate(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""After all this diffrents changes the network performances are not evolving as we can expect. So now let's change some network parameters like the optimizer and the learning rate to see if it has a bigger impact.

### Fifth network

In this attempt, we changed the RMSProp optimizer to Adam with the same learning rate as before.
"""

model = Sequential()
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=image_size))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.15))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.15))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(num_classes, activation='sigmoid'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=Adam(learning_rate=1e-3),
              metrics=['accuracy'])

# Taking only a part of the images to reduce the training time
train_x_final = np.asarray(train_x)[:5000]
test_x_final = np.asarray(test_x)
train_y_vector = to_categorical(train_y, num_classes=2)[:5000]
test_y_vector = to_categorical(test_y, num_classes=2)

# Train the model using the parameters defined earlier
history = model.fit(train_x_final.reshape(train_x_final.shape[0], train_x_final.shape[1], train_x_final.shape[2], 1), train_y_vector,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector),
                    callbacks = [callback])

# Finally, evaluate the model using the test data           
score = model.evaluate(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""So here we have a better results than the first network we've made ! The Adam .optimizer is more performant than RMSProp in general because it's using RMSProp
combined with an other method called Momentum (to be short it's a much stronger gradient descent algorithm)

### Sixth network

After changin the optimizer, let's study the impact of the learning rate on the network performances ! We know that if we decrease the learning rate, the network will take much more time to reach an optimum and if we increase it too much there is a risk of divergence. So it's a parameter that needs to be set
carefully. In general we use 1e-3 because it's a default value but here we'll test something else. There are some interesting method used to change the learning rate during the training, and we implemented one just below. We used the LearningRateScheduler callback to make this implementation.
"""

def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):
    """ Method that create a LearningRateScheduler with step decay schedule"""
    def schedule(epoch):
        return initial_lr * (decay_factor ** np.floor(epoch/step_size))
    
    return LearningRateScheduler(schedule)

lr_sched = step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=2)

model = Sequential()
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=image_size))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.15))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.15))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(num_classes, activation='sigmoid'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=Adam(learning_rate=1e-3),
              metrics=['accuracy'])

# Taking only a part of the images to reduce the training time
train_x_final = np.asarray(train_x)[:5000]
test_x_final = np.asarray(test_x)
train_y_vector = to_categorical(train_y, num_classes=2)[:5000]
test_y_vector = to_categorical(test_y, num_classes=2)

# Train the model using the parameters defined earlier
history = model.fit(train_x_final.reshape(train_x_final.shape[0], train_x_final.shape[1], train_x_final.shape[2], 1), train_y_vector,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector),
                    callbacks = [callback, lr_sched])

# Finally, evaluate the model using the test data           
score = model.evaluate(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""The network greatly improved this time ! We can conclude that evolving learning rate during training is a good point ! Let's use all the images during the training with this last version to see if it also improves our network !

### Test with every image
"""

# Taking all images
train_x_final = np.asarray(train_x)
test_x_final = np.asarray(test_x)
train_y_vector = to_categorical(train_y, num_classes=2)
test_y_vector = to_categorical(test_y, num_classes=2)

# Train the model using the parameters defined earlier
history = model.fit(train_x_final.reshape(train_x_final.shape[0], train_x_final.shape[1], train_x_final.shape[2], 1), train_y_vector,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector),
                    callbacks = [callback, lr_sched])

# Finally, evaluate the model using the test data           
score = model.evaluate(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""As we can expect the results are even better than before. Now the last test we've done is to adjust the training parameters to see which one are really improving the training of the network.

### Training parameters adjusting

We'll focus on the number of epochs and the batch_size because it's the two
parameter we can change without breaking everything. For the epochs we tested the following values: 40, 30, 20, 10 and for the batch_size we tested 256, 128, 64, 32. The test were made by changing those two parameters one by one to test all the combinations.
"""

batch_sizes = [256, 128, 64, 32] # Number of images in each batch
epochs = [40, 30, 20, 10] # Number of training steps

# Taking all images
train_x_final = np.asarray(train_x)
test_x_final = np.asarray(test_x)
train_y_vector = to_categorical(train_y, num_classes=2)
test_y_vector = to_categorical(test_y, num_classes=2)

for i in batch_sizes:
  for j in epochs:
    # Train the model using the parameters defined earlier
    history = model.fit(train_x_final.reshape(train_x_final.shape[0], train_x_final.shape[1], train_x_final.shape[2], 1), train_y_vector,
                        batch_size=i,
                        epochs=j,
                        verbose=1,
                        validation_data=(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector),
                        callbacks = [callback, lr_sched])

    # Finally, evaluate the model using the test data           
    score = model.evaluate(test_x_final.reshape(test_x_final.shape[0], test_x_final.shape[1], test_x_final.shape[2], 1), test_y_vector, verbose=0)
    print("Parameters: (batch_size: " + str(i) + ", epochs: " + str(j) + ")")
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])